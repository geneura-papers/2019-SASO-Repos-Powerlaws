\documentclass[conference]{IEEEtran}
\usepackage{url} 
\usepackage{cite}
\usepackage{amsmath,amssymb,amsfonts}
\usepackage{algorithmic}
\usepackage{graphicx}
\usepackage{textcomp}
\usepackage{xcolor}
\def\BibTeX{{\rm B\kern-.05em{\sc i\kern-.025em b}\kern-.08em
    T\kern-.1667em\lower.7ex\hbox{E}\kern-.125emX}}
\begin{document}

\title{Power laws in code repositories revisited
  \thanks{ This paper has been supported in part by
    project DeepBio (TIN2017-85727-C4-2-P)}
}

\author{\IEEEauthorblockN{B. Ortiz}
  \IEEEauthorblockA{Geneura Team, ETSIIT and CITIC, University of Granada (Spain)\\
{\tt bortiz@ugr.es}}
\and
\IEEEauthorblockN{J. J. Merelo}
\IEEEauthorblockA{Geneura Team, ETSIIT and CITIC, University of Granada (Spain)\\
{\tt jmerelo@ugr.es}}\\
}

<<setup, cache=FALSE,echo=FALSE,warning=FALSE,message=FALSE>>=
library(ggplot2)
library("ggfortify")
library(ggthemes)
library(dplyr)
library(TTR)
library(xtable)
                                        #use 
pref <- 'data/2017/lines_'
files <- c('ejabberd_3-erl hrl yrl escript ex exs','tensorflow_2-py cc h',
           'mojo_2-pl pm PL','tty_2-rb','cask_5-el py','webpack_2-js','language-perl6fe_4-coffee p6',
           'tpot_5-py','scalatra_2-scala','Moose_2-pl pm xs t','django_8-py','docker_2-go',
           'fission_4-go py js','vue_2-js','Dancer2_2-pl pm t','rakudo_4-pm pm6 pl pl6 nqp')

urls <- c('processone/ejabberd','tensorflow/tensorflow',
          'kraih/mojo','piotrmurach/tty','cask/cask','webpack/webpack','perl6/atom-language-perl6',
          'rhiever/tpot','scalatra/scalatra','moose/Moose','django/django','docker/docker',
          'fission/fission','vuejs/vue','PerlDancer/Dancer2','rakudo/rakudo')

languages <- c('erlang','Python','Perl','Ruby','Emacs Lisp','JavaScript',
               'CoffeeScript','Python','Scala','Perl','Python','Go','Go','JavaScript','Perl','Perl')

age <-  data.frame(Name = character(),
                   file = character(),
                   language = character(),
                   age = integer(),
                   Median = double(),
                   Mean = double(),
                   SD = double())
url.list <- list()
for (i in 1:length(files) ) {
    file.name = paste0(pref,files[i],'.csv')
    these.lines <-  read.csv(file.name)
    url.list[[file.name]] <- urls[i]
    age <- rbind( age,
                 data.frame(Name = urls[i],
                            file = file.name,
                            language = languages[i],
                            age = length(these.lines$Lines.changed),
                            Median =  as.double(median(these.lines$Lines.changed)),
                            Mean = as.double(mean(these.lines$Lines.changed)),
                            SD = as.double(sd(these.lines$Lines.changed) )))
}
summary <- age[order(age$age),]
lines <- list()
# Read again in order because I am useless in R
for (i in 1:length(summary$file) ) {
    lines[[i]] <-  read.csv(as.character(summary[[i,'file']]))
    lines[[i]]$url <- url.list[[summary[[i,'file']]]]
    lines[[i]]$SMA10 <- SMA(lines[[i]]$Lines.changed,n=10)
    lines[[i]]$SMA20 <- SMA(lines[[i]]$Lines.changed,n=20)
}
@ 

\maketitle

\begin{abstract}
Software development is an example of self-organized systems. Even if there can be team leaders, the growth and evolution of the code in repositories is mainly the result of self-organization.
\end{abstract}

%\keywords{Complex systems, self-organizing systems, self-organized
%  criticality, power laws, artificial life}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%   INTRODUCTION   %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Introduction}\label{introduction}

Self-organized criticality (SOC) \cite{bak1988self} is a condition
that describes complex systems in many different venues and
environments. The dynamics of systems in this state is sometimes
compared to that of a sand pile \cite{paczuski1996avalanche}, in the
sense that 
the actual shape tends to reach also a critical state, 
represented in the sand pile by a
critical slope, and a single grain of sand creates avalanches
unrelated to the frequency of grains falling. This pile of sand is
also a simple model of a self-organized system that captures many of
its main characteristics, but its behavior is connected to the
experience of software developers and other people using work
coordination mechanisms to produce something measurable, that experience
certain periods of stasis followed by {\em avalanches} of work, new
code or new paragraphs without an apparent origin. 

The anecdotal evidence gathered by the authors confirms this kind of events
happens in software development teams, mainly in an open source context. 
% Antonio - Is this properly justified (as an 'anecdotal evidence')? I guess it happens due to the unorganized/unexpected contributions that can appear in open source projects, right?
% They are not unorganized, and we are not justifying anything, just
% giving a motivation why people would thing this is worthy of study - JJ
As a matter of fact, several researchers have
proved that many projects match the features of a critical state,
\cite{wu2007empirical,gorshenev2004punctuated,Merelo2016:repomining,gao2014analysis},
which is 
attributed to a stigmergy process \cite{robles05}. Stigmergy implies
collaboration without direct contact between agents, but through the
environment. In this case, collaborators interact through the code
itself and through messages left, like pheromones, in other
communication media, such as Slack, email or an IRC chat
application, task assignment systems or mailing lists. 
% un revisor se queja de esta frase justo anterior. Quizás la vea larga, pero sí tiene sentido  - Pedro
% Expandida y dividida. - JJ
These are probably just some of the self-organization processes taking place,
but the fact is that SOC predicts that the system will reach a
critical state through self-organization given very few initial
conditions. In this
critical state there are specific dynamic behaviors, like small
changes in the code base provoking {\em avalanches} of changes of all sizes and long-distance
correlations that make a particular change in a file cause
further changes down the line in a regular pattern. 

Furthermore, the case for this
critical state in software repositories is supported by several macro measures that certify the absence of a particular
scale in the size of changes, that is, a {\em scale-free} structure
\cite{wu2007empirical,gorshenev2004punctuated,Merelo2016:repomining}, but in some cases they also exhibit \emph{long-range correlations} and \emph{pink noise} \cite{szendro2001pink} in the power spectral density, with
noise or variations with respect to the normal frequency
changing in a way that is inversely proportional to it, higher frequency
changes getting a smaller spectral density
\cite{Merelo2016:repomining}.

Even if the existence of this SOC state in repositories is well
established, the choice of repositories to study is limited to
long-running and big projects such as the Linux kernel. However, in
fact SOC should need few initial conditions to take place. This is why
in this paper we will look at all kind of repositories, with {\em
  ages} starting at a few months and sizes, looking for the three
characteristics of systems in a critical self-organized
state. Besides,  we take a new approach to analyzing the repositories,
different from the usual one that considers daily changes in size for
the whole repository; instead, we introduce a {\em natural} timeline which uses as a {\em
  tick} the commit itself, and consider either increases or decreases in size, not only increases. 
% Antonio - as it is usual? as it was previously done?
% Don't understand what you mean - JJ
In fact, a refactorization where a number of lines is changed might
not show up in daily changes because size in lines might not change, 
while it will have an impact in this one equal to the number of lines changed. 

One of the intentions in this paper is to try and prove that, in the same
way it happens in neural systems \cite{10.3389/fnsys.2014.00166}, the
self-organized state might be essential to the software development
process, as long as it is done through an application that allows
collaboration, for instance, as a repository managed by a source code
control system such as {\tt git}. In fact, some explanations have been offered via conservation laws \cite{6784340} and other usual complex network
mechanisms such as preferential attachment \cite{lin2015power}.

% Antonio - It is not clear the motivation, rather than just 'observe' if SOC happens. Please, explain what will be the good news for developers or so if SOC is detected. ;)
% That is enough motivation. We don't know in advance if it happens or
% noT - jj
%% It reminds me about the cathedral vs bazaar discussion 
%% Add something about it if you want - JJ
% At any rate, software projects can be considered complex systems and
% as such we will study them in this paper. 

After some initial exploration of the subject and developing the
tools needed to mine repositories on GitHub \cite{merelo16:self}, in
this paper we will study many different software repositories, all of
them in active development, trying to find out 
the telltale signs of self-organized criticality, as enumerated above. 
Previously we have applied these techniques to different types of repositories
\cite{Merelo2016:repomining,merelo16:slash,merelo16:self}, finding
they  have
this scale-free size property.
% Antonio - Tell a bit about the findings in those previous papers... Something like "SOC was detected in all the cases". Therefore the motivation will be clearer in this one if that happened in previous works. ;)
% And this will make the paper well over the 9 pages I so painfully
% managed to reduce it - JJ

% In this paper, we are going  
% to work on a total of 16 repositories, chosen among those that have at
% least a few hundred commits, 
% % Antonio - Better say "more than a hundred commits" or so
% independently of the actual time it has taken to
% make them. Since repositories hold many kind of different files,
% including screen art, documentation and configuration files for building tools, we have focused on
% the code written in the two or three main languages of the repository
% itself, since in fact the dynamics of documentation and build tools is
% probably completely different; these languages have been found using
% the GitHub tool that reveals the percentage of code written in every
% language. 
% I will leave this for methodology- JJ

Our intention in this line of research is to eventually find when the
change of phase to the critical state occurs and if there are any
other signs related to it. That is why, in this paper, we will focus
on the characterization of software teams, as reflected in the work
logs, that are actually in that state. 

% I have deleted the LaTeX code to include the number of sections as this style does not support them   [pedro]
% Thanks - JJ
Next, after presenting a brief state of the art, we will explain our
methodology followed by the results obtained, closing with our
conclusions. 


% --------------------------------------------------------


\section{State of the art}\label{soa}

As far as we know, there has not been a continuing line of research on
self-organized criticality in software projects or, for that matter,
any other kind of collaborative work teams. The main reason might be
that researchers have thoroughly
proved that several big software repositories seem to be in a SOC state,
\cite{wu2007empirical,gorshenev2004punctuated}, although some
not-so-big and not-really-software repositories seem to be in that
state too \cite{Merelo2016:repomining,merelo16:slash,merelo16:self};
in these last papers suggest, that since these repositories have different characteristics in terms
of the number of users, age and type of information they hold implies that
self-organization, as it should be expected, is achieved with relative
ease and due to factors not related to the nature of the content being
developed, but the way the interaction is done itself. 

Indeed, this state of self-organized criticality
quantitatively proves what has been already established via
qualitative analysis, the fact that in many successful software
projects, developers self-organize \cite{Crowston2007564}, 
which is
the preferred way of working in distributed and volunteer
teams \cite{crowston2012free}. In fact, this way of organization
matches our own experience in development of open source projects such
as \cite{ae09,2016arXiv160101607M}, which are developed mainly by one
or a few coders, helped sporadically by other coders that find an
error and fix it or adapt the code to particular situations. 
In fact, this self-organization has also been observed in similar
projects such as Wikipedia \cite{10.1371/journal.pone.0017333}

A critical state might eventually emerge from self-organization
given the necessary conditions. However, there has been no work going further and
proving this even in the case that the team is carried out by a small team and
on repositories that are not devoted to software development. And this
critical state is key to carry the system to adaptive success, as
defined by \cite{benbya2006toward}, which presents 
the seven mechanisms that create complexity as initially enunciated for
biological systems and apply them to information system
development. 

One of these principles, the principle of {\em change rate}, which appears in the
way of observation-orientation-decision-action, is very explicit in
open source development, where you usually observe a repository or
project for a certain time, then find if there is something to fix or
the way you can adapt it to your particular circumstances, decide what
to do and how to do it, and, eventually, act on your fork, resulting in a
pull request to the original repository. These activities can
effectively be defined as social, so that teamwork in software
repositories is embedded in a kind of social
network. \cite{valverde2007self}
have proved how this network grows under the tension of top-down or
hierarchical rules and self-organization emerges from the bottom,
which makes software development teams a complex social system, indicating that the self-organized critical state would be much
more pervasive than initially thought. 

In this paper we will mine software repositories and take the measures
that would confirm they are in a self-organized critical
state. Eventually, our intention is to try and find the point when
phases change, although in this paper we will focus in studying the
existence of a self-organized criticality state in the repositories under study.
Next we
will present the methodology used to choose those repositories and
mine their information. 


% --------------------------------------------------------


\section{Methodology}
\label{sec:method}

We have chosen 16 repositories in different states of development, and representing from
web frameworks such as Django to Atom plugins in the case of {\em
  language-perl6e}. The projects they host are written in many different languages, either
interpreted or compiled, and vary also in {\em professionalism}, from
a recently started Atom editor plugin for Perl6 to Docker, an open
platform for building and running distributed applications,  
created and maintained by a fully professional community. 

Repository mining was done during the months of January to March
2017, although the particular time of collection should not have an
influence on the results, as long as they have a minimum amount of
commits. 

The way we look at changes in the repository is novel and was used for
the first time in the previous technical reports
\cite{Merelo2016:repomining}. When looking to find SOC in repositories
you have to look at the timeline of changes in those
repositories. In most papers so far, this timeline was {\em natural},
using days or weeks, and the change used was the increase in size
after every daily work. 

We will introduce several changes to this usual methodology. The first one
is the timeline: we will use a discrete timeline formed by the
commits, with every commit counting as time=1. 

The second one is that we will work with the size of changes to a particular
set of files in the repository, selected via wildcards; we exclude other 
artifacts such as images or style files. To extract information
about changes to these files in the repositories, we analyze the
repository log using a Perl script to extract the size of the changes
that have been made to the considered files. This discards many
commits from the initial set, leaving some repositories with less than
100 from the initial number. The actual number of commits modifying
code might vary from repository to repository; however this ratio is
not considered important as long as the remaining number of commits is
enough. 

The third methodological change introduced in this paper is referred
to code deltas. Since changes in commit logs include both the insertion and deletion of lines in
committed files, the largest of these values is taken; in particular,
this means that the addition of all changes will not be equal to the
sum of 
the sizes of all files. A change in two lines will appear in a diff 
as
``2 insertions, 2 deletions'', adding up to 0; that is why we consider
the largest of these two values; the main reason for doing so is also
that in fact, the algorithm that computes changes in the repository
examines similitude in lines and counts changes in two lines as two
insertions and two deletions. There is no way to find out whether
there have been actually two lines added somewhere and two deleted
somewhere else, so in absence of that, we opt for the heuristic of
using the largest of these two values as change size. This analysis
method is more precise that the one used by other authors, 
which takes
into account only changes in size in the whole repository, and thus does
not include activities such as refactoring that consist mainly in
making changes in place. It also has the granularity of single commits
and not days or weeks, which are not as problem-specific as using the
number of commits.

We extract this information via a Perl script, that generates a {\tt
  .csv} file with a single column with the sequence of changes in size in the files of interest in each
repository. These data files, as well as the repositories where they have
been measured, are available with a free license in the repository
that also hosts this paper\footnote{\url{http://github.com/JJ/literaturame}}. 

Please remember that the $x$ axis for these sequences of changes,
shown smoothed in Figure \ref{fig:smoothie}, does not correspond to physical time,
but simply to the commit number, and then only in changes of the
{\em interesting}, that is, code files. In this sense, there is an important difference between
our research methodology, which considers atomic changes, to papers such
as \cite{herraiz2009statistical}, which take into account {\em daily}
changes. We think that examining discrete changes does not impose a
particular rhythm, namely, daily, on the changes, but lets the
repository expose its own rhythm; it also allows us to examine
slow-changing repositories such as what is usual in non-critical open
source projects, that can be static for
a long time to experience a burst of changes all of a sudden;
precisely these changes can indicate an {\em avalanche} that is a
symptom of the underlying self-organized criticality state and might
be better detected using commits. An {\em avalanche} caused by a small change 
happening in a day followed by a flurry of changes will appear as a big change 
if the time unit is bigger than the discrete commit. 

Once the information from the repositories has been extracted, we
proceed to analyze it in order to find clues about SOC behaviour. 

With that particular objetive in mind, we change the methodology used
in \cite{merelo2017self} for several reasons.

First of all, it is unrealist to think that a powerlaw distribution will
fit all our data. Then, our first step is to check what portion of the data
could be fitted with a powerlaw, or in other words, what is the minimal value 
(if there is one) from which the scaling relationship of the powerlaw begins.
This is a fair assumption since we are working with heavy-tailed distributions 
and our main interest is the behaviour of the tail of our data.

Moreover, as it is shown in papers like \cite{newman2005power, clauset2009power},
least square method is a poor way to proceed when estimatingour fitted powerlaw's
parameters. Instead, we are going to use a direct method describe in
\cite{clauset2009power} and impletmented in \cite{alstott2014powerlaw}.

Up to this point we have revisited our way of analyze powerlaw fitting and
the estimation of our parameters. However, there is a more deep question unanswered:
does our data really follow (in a statiscally relevant way) a powerlaw?
This kind of tests were lacking in \cite{merelo2017self} and they are relevant
since they usually offer an unbiased look of the data. 

Taking into account that our main question is wether a powerlaw is the best
description of our data, we choose to apply a comparative test that could 
evaluate if there are any alternative distribution that could have generated
our data with greater likelihood than a powerlaw. That is the main reason why
we choose to use a loglikehood ratio test implemented in \cite{alstott2014powerlaw}.

Finally, we offer a graphical view of these result different from \cite{merelo2017self}.
We are going to use the probability density function (PDF) for plotting. Due to the 
requirement of binning the data to this type of graphic, we are going to use a
logarithmic spacing, since it reduces the statistical errors in the tail in log-log 
plots at it is stated in \cite{newman2005power}.

% --------------------------------------------------------


\section{Results}
\label{res}
%
\begin{table*}[h!tb]
    \centering
<<commits, cache=FALSE,echo=FALSE>>=
kable.summary <- summary # Row names are useful
row.names(kable.summary) <- NULL
kable.summary$file <- NULL
kable( kable.summary,"latex",digits=2 )
@
\vspace*{1mm}
\caption{Summary of statistical measures for the software repositories
 we have analyzed here. {\tt Name} is the name of the repository in
 Github, {\tt age} is the total number of commits that affect the
 files under study, next is the main language the project uses, {\tt
   Mean} and {\tt Median} are computed over change sizes in number of
 lines, with {\tt SD} = standard deviation from the mean.\label{t:stat}}
\end{table*}
%
A summary of the statistical characteristics of the size of the commits,
in number of lines, is shown in Table \ref{t:stat}.


\begin{figure*}[h!tb]
  \centering
<<linecount,message=FALSE, fig.subcap=summary$Name, echo=FALSE,warning=FALSE,fig.height=4,out.width='.245\\linewidth'>>=
sizes.fit.df <- data.frame(Name = character(),
                           Coefficient = double(),
                           Intercept = double())
for (i in 1:length(lines) ) {
    by.lines <- group_by(lines[[i]],Lines.changed)
    lines.count <- summarize(by.lines, count=n())
    sizes.fit <- lm(log(1+lines.count$Lines.changed) ~ log(lines.count$count))
    repo <- strsplit(paste(summary[[1]][i],""),"_",fixed=T)
    sizes.fit.df <- rbind( sizes.fit.df,
                          data.frame( Name = repo[[1]][1],
                                     Intercept = summary(sizes.fit)$coefficients[1],
                                     Coefficient = summary(sizes.fit)$coefficients[2] ))
    print(ggplot(lines.count, aes(x=Lines.changed, y=count))+geom_point()+scale_x_log10()+scale_y_log10()+stat_smooth() + theme(legend.position="none",axis.title.x=element_blank(),axis.title.y=element_blank()) + ggtitle(lines[[i]]$url))
}
@ 
\caption{Number of changes of that size vs. lines changed in a log-log scale. The
  blue line and gray zone is a smoothed version of the chart. \label{fig:changes}}
\end{figure*}

These distributions can, in fact, be linearly fit to a log-log distribution with coefficients
shown in Table \ref{t:sizes}. 
%
\begin{table}[h!]
    \centering
<<sizes, cache=FALSE,echo=FALSE>>=
kable( sizes.fit.df,"latex" )
@
\vspace*{2mm}
\caption{Summary of coefficients of the linear models adjusting number
  of change size occurrences vs. size.\label{t:sizes}}
\end{table}
%


\begin{figure*}[h!tb]
  \centering
<<powerlaw,message=FALSE, fig.subcap=summary$Name,echo=FALSE,warning=FALSE,fig.height=4,out.width='.115\\linewidth'>>=
zipf.fit.df <- data.frame(Name = character(),
                          Coefficient = double(),
                          Intercept = double())
for (i in 1:length(lines) ) {
    sorted.lines <- data.frame(x=1:length(lines[[i]]$Lines.changed),Lines.changed=as.numeric(lines[[i]][order(-lines[[i]]$Lines.changed),]$Lines.changed))
    print(ggplot()+geom_point(data=sorted.lines,aes(x=x,y=Lines.changed))+scale_y_log10())
    sorted.lines.no0 <- sorted.lines[sorted.lines$Lines.changed>0,]
    repo <- strsplit(paste(summary[[1]][i],""),"_")
    zipf.fit <- lm(log(sorted.lines.no0$Lines.changed) ~ sorted.lines.no0$x)
    zipf.fit.df <- rbind( zipf.fit.df,
                         data.frame( Name = repo[[1]][1],
                                    Intercept = summary(zipf.fit)$coefficients[1],
                                    Coefficient = summary(zipf.fit)$coefficients[2] ))
}
@ 
\caption{Zipf, that is, ranked change sizes with logarithmic $y$ axis for the 16 repositories analyzed.\label{fig:zipf}}
\end{figure*}


\begin{table}[h!tb]
    \centering
<<zipf, cache=FALSE,echo=FALSE>>=
kable( zipf.fit.df,"latex" )
@
\vspace*{2mm}
\caption{Summary of Zipf coefficients of the linear models.\label{t:zipf}}
\end{table}
%


% --------------------------------------------------------


\section{Conclusions}\label{conc}


\bibliographystyle{apalike}
\bibliography{geneura,biblio}

\end{document}
